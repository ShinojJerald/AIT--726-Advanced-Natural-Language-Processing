{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjXKZ2xBiqI2"
   },
   "source": [
    "\n",
    "<img src=\"https://www2.gmu.edu/sites/all/modules/features/feature_core_theme/templates/resources/images/mason-logo.png \" alt=\"GMU Logo\" title=\"George Mason University\" />\n",
    "<hr style =\"color:#99CC99\">\n",
    "    \n",
    "<h2 style=\"font-family:Helvetica; color:#006633;\">Programming Assignment # 1</h2>\n",
    "<h3 style=\"font-family:Helvetica; color:#006633;\"> Sentiment Classification with Naïve Bayes and Logistic Regression</h3>\n",
    "\n",
    "<p style=\"font-family:Helvetica; font-size:1.5em;\"> \n",
    "Authors: Team 1 - Shinoj Jerald Bounaventure Kumar Jeronmary, Yitong Li, Anh Nguyen and Nina Nnamani<br>\n",
    "Course Professor: Dr. Lindi Liao <br>\n",
    "Course Name: Natural Language Processing <br>\n",
    "Course Name and Section#: AIT 726-001<br>\n",
    "University Name: George Mason University<br>\n",
    "Date: October 4, 2020    <br>\n",
    "</p>    \n",
    "<hr style =\"color:#99CC99\" width=\"75%\">\n",
    "<p style=\"font-family:Helvetica; font-size:1.4em;\"> \n",
    "Description: This program implements a \"from scratch\" Naïve Bayes and a Logistic Regression classifier for sentiment classification of airline review tweets. \n",
    " <br>\n",
    "    \n",
    "<p style=\"font-family:Helvetica; font-size:1.4em;\"> \n",
    "Instructions: This program is presented as a jupyter notebook and requires all packages in the \"import packages\" section to be installed prior to running the code to avoid errors. All training and testing tweet texts, the notebook and html of outputs are included in the folder, and as such the code can be run directly from it. Given that the coding for each classifer was split within the Team, the functions created for each step of the classifers developnments are documented with comments in the notebook. First is Naïve Bayes, followed by Logistic Regression. An html file of program of all outputs are included in this submission. The following overarching featurs are implemented:<br>\n",
    "\n",
    "<p style=\"font-family:Helvetica; font-size:1.2em;\"> \n",
    "1) Creates vocabulary<br>\n",
    "2) Extracts features/ Bag of words representations <br>\n",
    "3) Trains the classifers<br>\n",
    "4) Evaluates the test sets<br>\n",
    "5) Reports accuracy score and confusion matrix of classifiers performance<br>\n",
    "<br> \n",
    "    \n",
    "        \n",
    "<p style=\"font-family:Helvetica; font-size:1.4em;\"> \n",
    "References: The following listed sources provided some insight on code techniques that were partially adapted.\n",
    "    \n",
    "https://github.com/ChanchalKumarMaji/Natural-Language-Processing-Specialization-deeplearning.ai/tree/master/Natural%20Language%20Processing%20with%20Classification%20and%20Vector%20Spaces/Week%201 \n",
    "    \n",
    "https://streamsql.io/blog/sentiment-analysis\n",
    "    \n",
    "https://stackoverflow.com/questions/4145451/using-a-regular-expression-to-replace-upper-case-repeated-letters-in-python-with \n",
    "    \n",
    "https://www.nltk.org/_modules/nltk/tokenize/casual.html\n",
    " <br>\n",
    "\n",
    "<hr style =\"color:#99CC99\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVpCp83ENr7r"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QszPJ6VUNr7s"
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import os\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import collections\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import xml\n",
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#captures runtime\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xd1KzhgNr7v"
   },
   "source": [
    "## Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEWaFBsZuaTF"
   },
   "source": [
    "### Removes mark-up, normalizes capitalized first letter and removed emoticons/emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Qoa68ypENr7w"
   },
   "outputs": [],
   "source": [
    "def normalize_case(s):    \n",
    "    '''\n",
    "    Paramaeter: Word to be normalized\n",
    "    Converts words with capitalized first letters in to lower case.\n",
    "    '''\n",
    "    if(not s.isupper()):\n",
    "        return s.lower()\n",
    "    else:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vTsHGO2gNr7z"
   },
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string) # no emoji\n",
    "\n",
    "def remove_tags(s):\n",
    "    '''\n",
    "    Paramaeter: Word to be normalized\n",
    "    Removes HTML tags\n",
    "    '''\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OhpFRejNr72"
   },
   "source": [
    "### Creates vocabulary by incorporating function and tokenizing text for unstemmed and stemmed.  Extract features by getting frequency counts in each document and binary representation count for \"in\" or \"not\" in document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4MTEaaakNr72"
   },
   "outputs": [],
   "source": [
    "def count_words(rootdir):\n",
    "    '''\n",
    "    Parameter: root directory\n",
    "    \n",
    "    The funtion collects the training files. Tokenizes text into words. Creates stemmed vocabulary and \n",
    "    Counts the the occurance of each word in each class (positve and negative).\n",
    "    '''    \n",
    "    #Port Stemmer for stemming vocabulary\n",
    "    ps = PorterStemmer()\n",
    "    vocab=[]\n",
    "    total_words=0\n",
    "    stemmed_vocab=[]\n",
    "    prior=0\n",
    "    binary_count={}\n",
    "    stemmed_binary_count={}\n",
    "    #For each directory in the path\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        #For each file in the directory\n",
    "        for file in files:\n",
    "            f=open(rootdir+file,'r',encoding='utf-8') #use the absolute URL of the file\n",
    "            lines = f.readlines()\n",
    "            S = set()\n",
    "            #For each line in the file\n",
    "            for line in lines:\n",
    "                #Tokenize the words using word tokenize of nltk\n",
    "                document=word_tokenize(line)\n",
    "                #For each word in the document \n",
    "                for i in range(0,len(document)):\n",
    "                    #Normalize case for the word, convert capitalized letter to lower case\n",
    "                    document[i]=normalize_case(document[i])\n",
    "                    #Remove HTML tags\n",
    "                    document[i]=remove_tags(document[i])\n",
    "                    # Remove emoji\n",
    "                    document[i]=remove_emoji(document[i])\n",
    "                    if(document[i]!=''):\n",
    "                        total_words+=1\n",
    "                        #Stem the words and append to stemmed list\n",
    "                        stemmed_vocab.append(ps.stem(document[i]))\n",
    "                        if(not document[i] in S):\n",
    "                            #Calculate stemmed binary count\n",
    "                            stemmed_binary_count[ps.stem(document[i])]=stemmed_binary_count.get(ps.stem(document[i]),0)+1\n",
    "                            S.add(ps.stem(document[i]))\n",
    "                        vocab.append(document[i])\n",
    "                        if(not document[i] in S):\n",
    "                            #Calculate  binary count\n",
    "                            binary_count[document[i]]=binary_count.get(document[i],0)+1\n",
    "                            S.add(document[i])\n",
    "            f.close()\n",
    "            prior+=1\n",
    "    #Count frequency of words from respective vocabs\n",
    "    \n",
    "    count=dict(collections.Counter(vocab))\n",
    "    stemmed_count=dict(collections.Counter(stemmed_vocab))\n",
    "    return [count,stemmed_count,prior,binary_count,stemmed_binary_count,total_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1YT_6007Nr76"
   },
   "outputs": [],
   "source": [
    "# Return the unique words from two lists \n",
    "def Union(lst1, lst2): \n",
    "    final_list = list(set(lst1) | set(lst2)) \n",
    "    return final_list \n",
    "\n",
    "def nb():\n",
    "    '''\n",
    "    The funtion collects the count of non stemmed and stemmed vocabulary and assigns it to global variables.\n",
    "    '''    \n",
    "    global pos_count\n",
    "    global neg_count\n",
    "    global unique_pos_words\n",
    "    global unique_neg_words\n",
    "    global unique_words\n",
    "    global stemmed_pos_count\n",
    "    global stemmed_neg_count\n",
    "    global positive_prior\n",
    "    global negative_prior\n",
    "    global pos_stemmed_binary_count\n",
    "    global neg_stemmed_binary_count\n",
    "    \n",
    "    global pos_binary_count\n",
    "    global neg_binary_count\n",
    "    \n",
    "    global total_pos_words\n",
    "    global total_neg_words\n",
    "    \n",
    "    #Calculate parameters for positve documents\n",
    "    rootdir= \"train/positive/\" \n",
    "    count=count_words(rootdir)\n",
    "    pos_count=count[0]\n",
    "    stemmed_pos_count=count[1]\n",
    "    unique_pos_words=len(pos_count)\n",
    "    positive_prior=count[2]\n",
    "    pos_binary_count=count[3]\n",
    "    pos_stemmed_binary_count=count[4]\n",
    "    total_pos_words=count[5]\n",
    "    \n",
    "    #Calculate parameters for negative documents\n",
    "    rootdir= \"train/negative/\"\n",
    "    count=count_words(rootdir)\n",
    "    neg_count=count[0]\n",
    "    stemmed_neg_count=count[1]\n",
    "    unique_neg_words=len(neg_count)\n",
    "    negative_prior=count[2]\n",
    "    neg_binary_count=count[3]\n",
    "    neg_stemmed_binary_count=count[4]\n",
    "    total_neg_words=count[5]\n",
    "    \n",
    "    # Calculate the unique number of words from the training set\n",
    "    unique_words=len(Union(list(neg_count.keys()),list(pos_count.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKIeP92WNr78"
   },
   "source": [
    "### Trainings and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yuXxGFM_Nr79"
   },
   "outputs": [],
   "source": [
    "def get_test(rootdir):\n",
    "    '''\n",
    "    The funtion collects the test data. Creates Bag of words and\n",
    "    stemmed vocabulary.\n",
    "    '''    \n",
    "    ps = PorterStemmer()\n",
    "    document=[]\n",
    "    tokenized_document=[]\n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            f=open(rootdir+file,'r',encoding='utf-8') #use the absolute URL of the file\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                tokenized=word_tokenize(line)\n",
    "                final=[]\n",
    "                tokenized_final=[]\n",
    "                for i in range(0,len(tokenized)):\n",
    "                    tokenized[i]=normalize_case(tokenized[i])\n",
    "                    tokenized[i]=remove_tags(tokenized[i])\n",
    "                    tokenized[i]=remove_emoji(tokenized[i])\n",
    "                    final.append(tokenized[i])\n",
    "                    tokenized_final.append(ps.stem(tokenized[i]))\n",
    "                document.append(final)\n",
    "                tokenized_document.append(tokenized_final)\n",
    "            test_files=document\n",
    "            stemmed_test_files=tokenized_document\n",
    "    f.close()\n",
    "    return [test_files,stemmed_test_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c3BhkUmMNr8A"
   },
   "outputs": [],
   "source": [
    "nb()\n",
    "def classify(test_files,stemmed_test_files):\n",
    "    '''\n",
    "    The function first calculates likelyhood and prior for each word. And then classify test documents based on these values\n",
    "\n",
    "    '''\n",
    "    doc_classification=[]\n",
    "    binary_doc_classification=[]\n",
    "    #Classify test documents using non-stemmed vocabulary\n",
    "    for doc in test_files:\n",
    "        pos_word_likelyhood=0\n",
    "        neg_word_likelyhood=0\n",
    "        pos_binary_likelyhood=1\n",
    "        neg_binary_likelyhood=1\n",
    "\n",
    "        for word in doc:\n",
    "            #Calculate the likeleyhood for each word appeared in positive and negative documents\n",
    "            pos_word_likelyhood=pos_word_likelyhood+np.log((pos_count.get(word,0)+1)/(total_pos_words+unique_words))\n",
    "            neg_word_likelyhood=neg_word_likelyhood+np.log((neg_count.get(word,0)+1)/(total_neg_words+unique_words))\n",
    "            \n",
    "            # Binary\n",
    "            #Calculate the likeleyhood for each word appeared in positive and negative documents\n",
    "            if(word in pos_binary_count):\n",
    "                pos_binary_likelyhood=pos_binary_likelyhood*(((pos_binary_count.get(word,0)+1)/(len(pos_binary_count)+len(pos_binary_count)+len(neg_binary_count))))\n",
    "            if(word in neg_binary_count):\n",
    "                neg_binary_likelyhood=neg_binary_likelyhood*(((neg_binary_count.get(word,0)+1)/(len(neg_binary_count)+len(neg_binary_count)+len(pos_binary_count))))\n",
    "        \n",
    "        #Calculate posterior\n",
    "        pos_class=(pos_word_likelyhood)+np.log(positive_prior/(positive_prior+negative_prior))\n",
    "        neg_class=(neg_word_likelyhood)+np.log(negative_prior/(positive_prior+negative_prior))\n",
    "        \n",
    "        #Classify documents based on calculated values\n",
    "        if(pos_class > neg_class):\n",
    "            doc_classification.append('pos')\n",
    "        elif(pos_class < neg_class):\n",
    "            doc_classification.append('neg')\n",
    "        if(pos_binary_likelyhood>neg_binary_likelyhood):\n",
    "            binary_doc_classification.append('pos')\n",
    "        else:\n",
    "            binary_doc_classification.append('neg')\n",
    "                       \n",
    "    #Classify test documents using stemmed vocabulary\n",
    "    stemmed_doc_classification=[]\n",
    "    binary_doc_classification_stemmed=[]\n",
    "    for doc in stemmed_test_files:\n",
    "        pos_word_likelyhood=0\n",
    "        neg_word_likelyhood=0\n",
    "        pos_stemmed_binary_likelyhood=1\n",
    "        neg_stemmed_binary_likelyhood=1\n",
    "        \n",
    "        for word in doc:\n",
    "                \n",
    "            pos_word_likelyhood=pos_word_likelyhood+np.log((stemmed_pos_count.get(word,0)+1)/(total_pos_words+unique_words))\n",
    "            neg_word_likelyhood=neg_word_likelyhood+np.log((stemmed_neg_count.get(word,0)+1)/(total_neg_words+unique_words))\n",
    "            \n",
    "            if(word in pos_stemmed_binary_count):\n",
    "                pos_stemmed_binary_likelyhood=pos_stemmed_binary_likelyhood*(((pos_stemmed_binary_count.get(word,0)+1)/(len(pos_stemmed_binary_count)+len(pos_stemmed_binary_count)+len(neg_stemmed_binary_count))))\n",
    "            if(word in neg_stemmed_binary_count):\n",
    "                neg_stemmed_binary_likelyhood=neg_stemmed_binary_likelyhood*(((neg_stemmed_binary_count.get(word,0)+1)/(len(neg_stemmed_binary_count)+len(neg_stemmed_binary_count)+len(pos_stemmed_binary_count))))\n",
    "        \n",
    "        pos_class=(pos_word_likelyhood)+np.log(positive_prior/(positive_prior+negative_prior))\n",
    "        neg_class=(neg_word_likelyhood)+np.log(negative_prior/(positive_prior+negative_prior))\n",
    "        if(pos_class > neg_class):\n",
    "            stemmed_doc_classification.append('pos')\n",
    "        elif(pos_class <= neg_class):\n",
    "            stemmed_doc_classification.append('neg')\n",
    "        if(pos_stemmed_binary_likelyhood>neg_stemmed_binary_likelyhood):\n",
    "            binary_doc_classification_stemmed.append('pos')\n",
    "        else:\n",
    "            binary_doc_classification_stemmed.append('neg')\n",
    "    return [doc_classification,binary_doc_classification,stemmed_doc_classification,binary_doc_classification_stemmed]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PyuFhHgSNr8C",
    "outputId": "aea74a0f-1f80-4dc0-b676-c2f8dfe8f602"
   },
   "outputs": [],
   "source": [
    "# Use function \"get_test\" to get Bag of words and stemmed vocabulary for POSITIVE test set\n",
    "# root directory is shown for data stored on the desktop\n",
    "test_files_pos=get_test(\"test/positive/\")[0]\n",
    "stemmed_test_files_pos=get_test(\"test/positive/\")[1]\n",
    "\n",
    "# Use function get_test to get Bag of words and stemmed vocabulary for NEGATIVE test set\n",
    "test_files_neg=get_test(\"test/negative/\")[0]\n",
    "stemmed_test_files_neg=get_test(\"test/negative/\")[1]\n",
    "\n",
    "# Use function \"classify\" to classify tes set based on combinations of \n",
    "# no stemming + frequency count, gold standard POSITIVE\n",
    "no_stemming_frequency_count_pos =classify(test_files_pos,stemmed_test_files_pos)[0]\n",
    "# no stemming + binary, gold standard POSITIVE\n",
    "no_stemming_binary_pos=classify(test_files_pos,stemmed_test_files_pos)[1]\n",
    "# stemming + frequency count, gold standard POSITIVE\n",
    "stemming_frequency_count_pos=classify(test_files_pos,stemmed_test_files_pos)[2]\n",
    "# stemming + binary, gold standard POSITIVE\n",
    "stemming_binary_pos=classify(test_files_pos,stemmed_test_files_pos)[3]\n",
    "\n",
    "# no stemming + frequency count, gold standard NEGATIVE\n",
    "no_stemming_frequency_count_neg =classify(test_files_neg,stemmed_test_files_neg)[0]\n",
    "# no stemming + binary, gold standard NEGATIVE\n",
    "no_stemming_binary_neg=classify(test_files_neg,stemmed_test_files_neg)[1]\n",
    "# stemming + frequency count, gold standard NEGATIVE\n",
    "stemming_frequency_count_neg=classify(test_files_neg,stemmed_test_files_neg)[2]\n",
    "# stemming + binary, gold standard NEGATIVE\n",
    "stemming_binary_neg=classify(test_files_neg,stemmed_test_files_neg)[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqFvhn8WNr8F"
   },
   "source": [
    "### Accuracy Score and Confusion Matrix of Naïve Bayes Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cVF7pXwLtssC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy  0.8162878787878788\n",
      "Confusion Matrix:- \n",
      "[[1088.  669.]\n",
      " [ 107. 2360.]]\n",
      "Accuracy  0.4786931818181818\n",
      "Confusion Matrix:- \n",
      "[[ 451. 1458.]\n",
      " [ 744. 1571.]]\n",
      "Accuracy  0.842092803030303\n",
      "Confusion Matrix:- \n",
      "[[1073.  545.]\n",
      " [ 122. 2484.]]\n",
      "Accuracy  0.6633522727272727\n",
      "Confusion Matrix:- \n",
      "[[ 233.  460.]\n",
      " [ 962. 2569.]]\n"
     ]
    }
   ],
   "source": [
    "def metrics(gold_pos,gold_neg): # classified documents with gold standard positive and negative, respectively\n",
    "    '''\n",
    "    The function calculates accuracy and confusion matrix.\n",
    "    '''\n",
    "    arr=np.ndarray(shape=(2,2), dtype=float, order='F')\n",
    "    arr.fill(0)\n",
    "    for i in range(0,len(gold_pos)):\n",
    "        if (gold_pos[i]=='pos'):\n",
    "            arr[0][0]=arr[0][0]+1\n",
    "        if (gold_pos[i]=='neg'):\n",
    "            arr[1][0]=arr[1][0]+1            \n",
    "    for i in range(0,len(gold_neg)):\n",
    "        if (gold_neg[i]=='pos'):\n",
    "            arr[0][1]=arr[0][1]+1\n",
    "        if (gold_neg[i]=='neg'):\n",
    "            arr[1][1]=arr[1][1]+1            \n",
    "            \n",
    "    accuracy=(arr[0][0]+arr[1][1])/(arr[0][0]+arr[1][1]+arr[0][1]+arr[1][0])\n",
    "    print(\"Accuracy \",accuracy)\n",
    "    print(\"Confusion Matrix:- \")\n",
    "    print(arr)\n",
    "    \n",
    "# Compute accuracy and con for all four combinations\n",
    "# no stemming + frequency count\n",
    "metrics(no_stemming_frequency_count_pos,no_stemming_frequency_count_neg)\n",
    "# no stemming + binary\n",
    "metrics(no_stemming_binary_pos,no_stemming_binary_neg)\n",
    "# stemming + frequency count\n",
    "metrics(stemming_frequency_count_pos,stemming_frequency_count_neg)\n",
    "# stemming + binary\n",
    "metrics(stemming_binary_pos,stemming_binary_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCUPo9O-H9v6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hujXoKxjNr8I"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cukIuNVXNr8J"
   },
   "source": [
    "### Removes mark-up, normalizes capitalized first letter and removed emoticons/emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "a2UlXDYtNr8J"
   },
   "outputs": [],
   "source": [
    "#reading the text data\n",
    "pos_train = os.listdir(\"train/positive/\")\n",
    "neg_train = os.listdir(\"train/negative/\")\n",
    "pos_test = os.listdir(\"test/positive/\")\n",
    "neg_test = os.listdir(\"test/negative/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VoCY275QNr8L"
   },
   "outputs": [],
   "source": [
    "#Removing Emoticons\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string) # no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Km37up8rNr8N"
   },
   "outputs": [],
   "source": [
    "#Removing HTML tags\n",
    "def remove_tags(text):\n",
    "    s = re.sub(r'<[^>]+>', '', text)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vmmNjQTNr8P"
   },
   "source": [
    "### Creates vocabulary by incorporating function and tokenizing text for unstemmed and stemmed.  Extract features for Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WI5f04-zNr8P"
   },
   "outputs": [],
   "source": [
    "# Cleaning and adding the data\n",
    "def create_list(dir,type,type1):\n",
    "    return_list=[]\n",
    "    for i in range(0, 500):\n",
    "        file1 = open(type+\"/\"+type1+\"/\" + dir[i])\n",
    "        try:\n",
    "            text = file1.read()\n",
    "            text = remove_tags(text)\n",
    "            text = remove_emoji(text)\n",
    "            return_list.append(text)\n",
    "        except UnicodeDecodeError:\n",
    "            k=0\n",
    "        file1.close()\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0BgJny3DNr8S"
   },
   "outputs": [],
   "source": [
    "#Creating dataframe with positive and negative tweets\n",
    "def createDataFrame(pos_train_list,neg_train_list):\n",
    "    df1 = pd.DataFrame(neg_train_list)\n",
    "    target2 = [0] * len(neg_train_list)\n",
    "    df1[\"target\"] = target2\n",
    "    df1 = df1.rename(columns={0: \"text\"})\n",
    "#Giving positive tweets as 1 and negative tweets as 0\n",
    "    df = pd.DataFrame(pos_train_list)\n",
    "    target1 = [1] * len(pos_train_list)\n",
    "    df[\"target\"] = target1\n",
    "    df = df.rename(columns={0: \"text\"})\n",
    "    \n",
    "#Data is getting shuffled here\n",
    "    data = pd.concat([df, df1])\n",
    "    #data = data.sample(frac=1)\n",
    "    x = list(data[\"text\"])\n",
    "    y=np.array(data[\"target\"])\n",
    "\n",
    "    return x,y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ToH5V_MlNr8T"
   },
   "outputs": [],
   "source": [
    "# Tokenizing the text\n",
    "def clean(text):\n",
    "\n",
    "    vocab=[]\n",
    "    for j in word_tokenize(text):\n",
    "        if (j != ''):\n",
    "            if not j.islower() and not j.isupper():\n",
    "                j = j.lower()\n",
    "            vocab.append(j)\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "5c7uS5q1Nr8W"
   },
   "outputs": [],
   "source": [
    "# Tokenizing the text and stemming it\n",
    "def cleaningStemmed(text):\n",
    "\n",
    "    ps = PorterStemmer()\n",
    "    vocabulary_stemmed=[]\n",
    "    for j in word_tokenize(text):\n",
    "        if (j != ''):\n",
    "            if not j.islower() and not j.isupper():\n",
    "                j = j.lower()\n",
    "            vocabulary_stemmed.append(ps.stem(j))\n",
    "\n",
    "    return vocabulary_stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg8ooaQeNr8X"
   },
   "source": [
    "### Training Functions: Initialize weights, cross-entropy as the loss function and stochastic gradient ascent as the optimization algorithm, sets sigmoid threshold.  Predict labels for each sample. Compute the cross entropy and gradient of predictions against the gold standard labels. Updates weights with the gradient of the score function using learning rate. Iterate until performance converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sVE9XzUWNr8Y"
   },
   "outputs": [],
   "source": [
    "#Defining sigmoid function\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Defining the gradient function for 500 iterations \n",
    "def gradient_descent(X, y, params, learning_rate, iterations):\n",
    "\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "\n",
    "    for i in range(iterations):\n",
    "        params = params - (learning_rate/m) * (X.T @ (sigmoid(X @ params) - y))\n",
    "        cost_history[i] = compute_cost(X, y, params)\n",
    "\n",
    "    return (cost_history, params)\n",
    "\n",
    "\n",
    "# The objective cost function\n",
    "def compute_cost(X, y, theta):\n",
    "\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    cost = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))\n",
    "\n",
    "    return cost\n",
    "\n",
    "#Defining the regularised gradient function for 500 iterations\n",
    "def gradient_descent_reg(X, y, params, learning_rate, iterations, lmbda):\n",
    "\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "\n",
    "    for i in range(iterations):\n",
    "        params = params - (learning_rate/m) * (X.T @ (sigmoid(X @ params) - y))\n",
    "        cost_history[i] = compute_cost_reg(X, y, params, lmbda)\n",
    "\n",
    "    return (cost_history, params)\n",
    "\n",
    "# The objective regularised cost function\n",
    "def compute_cost_reg(X, y, theta, lmbda):\n",
    "\n",
    "    m = len(y)\n",
    "    h = sigmoid(X @ theta)\n",
    "    temp = theta\n",
    "    cost = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lmbda / (2 * m)) * np.sum(np.square(temp))\n",
    "\n",
    "    return cost\n",
    "\n",
    "# Final prediction function\n",
    "def predict(X, params):\n",
    "    return np.round(sigmoid(X @ params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ4PdpIxNr8Z"
   },
   "source": [
    "### Vectorizing function does so by counts and also does TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "n5FYXUcBNr8Z"
   },
   "outputs": [],
   "source": [
    "# Vectorizing based user input\n",
    "def vectorizer(X,vectorArr,dict_vocab,vectorType,row,col):\n",
    "\n",
    "    if (vectorType==1):\n",
    "        for i in range(0, len(X)):\n",
    "            for j in X[i]:\n",
    "                if j in dict_vocab:\n",
    "                    vectorArr[i, dict_vocab[j]] += 1\n",
    "\n",
    "\n",
    "        idf= np.zeros((row, col), dtype=np.int64)\n",
    "        for i in range(0,len(vectorArr)):\n",
    "            for j in range(0,col):\n",
    "                if vectorArr[i][j] > 0:\n",
    "                    idf[i][j]= math.log10(row / float(vectorArr[i][j]))\n",
    "\n",
    "                else:\n",
    "                    idf[i][j]=0\n",
    "        vectorArr=np.multiply(vectorArr, idf)\n",
    "\n",
    "    elif (vectorType==2):\n",
    "        for i in range(0, len(X)):\n",
    "            for j in X[i]:\n",
    "                if j in dict_vocab:\n",
    "                    vectorArr[i, dict_vocab[j]] += 1\n",
    "\n",
    "    else:\n",
    "        for i in range(0, len(X)):\n",
    "            for j in X[i]:\n",
    "                if j in dict_vocab:\n",
    "                    vectorArr[i,dict_vocab[j]]=1\n",
    "\n",
    "    return vectorArr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HmqncDV3AKiG"
   },
   "source": [
    "### Evaluation of models: F1 score, accuracy and confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "rvkPZ-ZxAJIA"
   },
   "outputs": [],
   "source": [
    "def accuracy_score(actual, predicted):\n",
    "  correct = 0\n",
    "  for i in range(len(actual)):\n",
    "    #compare the element at index i between actual result vs prediction\n",
    "    if actual[i] == predicted[i]:\n",
    "      correct += 1\n",
    "  return correct / float(len(actual)) * 100.0\n",
    "def confusion_matrix(actual, predicted):\n",
    "#inputs are binary arrays of actual results in test data and prediction from the model\n",
    "  actual_results = pd.Series(actual, name='Actual')\n",
    "  predictions = pd.Series(predicted, name='Predicted')\n",
    "  #cros table function in pandas\n",
    "  df_confusion = pd.crosstab(actual_results,predictions)\n",
    "  return df_confusion\n",
    "def f1_score(matrix):\n",
    "  #confusion matrix as printed using confusion_matrix() function. [1][1] for TP, [1][0] for FP, [0][0] for TN, [0][1] for FN\n",
    "  precision=matrix[1][1]/(matrix[1][1]+matrix[1][0])\n",
    "  recall=matrix[1][1]/(matrix[1][1]+matrix[0][1])\n",
    "  f1=2/((1/precision)+(1/recall))\n",
    "    #accuracy is same as the result in accuracy_score\n",
    "    #accuracy=100* (matrix[0][0]+matrix[1][1])/(matrix[1][1]+matrix[1][0]+matrix[0][1]+matrix[0][0])\n",
    "  print(\"Precision: \",\"{:.2f}\".format(precision),\"\\nRecall: \",\"{:.2f}\".format(recall), \"\\nF1 score (micro):\",\"{:.2f}\".format(f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWRzY4nwNr8b"
   },
   "source": [
    "### Main function to combine functions for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3ywJQ5pyNr8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Clean:  Stemmed\n",
      "Vectorization:   TF-IDF vectorizer\n",
      "LinReg costfunction:  Regularized\n",
      "Precision:  0.84 \n",
      "Recall:  0.79 \n",
      "F1 score (micro): 0.82\n",
      "None\n",
      "Accuracy:  82.55%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          421   70\n",
      "1           98  374\n",
      "Data Clean:  Stemmed\n",
      "Vectorization:   TF-IDF vectorizer\n",
      "LinReg costfunction:  Not regularized\n",
      "Precision:  0.84 \n",
      "Recall:  0.79 \n",
      "F1 score (micro): 0.82\n",
      "None\n",
      "Accuracy:  82.55%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          421   70\n",
      "1           98  374\n",
      "Data Clean:  Stemmed\n",
      "Vectorization:   Count vectorizer\n",
      "LinReg costfunction:  Regularized\n",
      "Precision:  0.82 \n",
      "Recall:  0.75 \n",
      "F1 score (micro): 0.78\n",
      "None\n",
      "Accuracy:  79.85%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          415   76\n",
      "1          118  354\n",
      "Data Clean:  Stemmed\n",
      "Vectorization:   Count vectorizer\n",
      "LinReg costfunction:  Not regularized\n",
      "Precision:  0.82 \n",
      "Recall:  0.75 \n",
      "F1 score (micro): 0.78\n",
      "None\n",
      "Accuracy:  79.85%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          415   76\n",
      "1          118  354\n",
      "Data Clean:  Stemmed\n",
      "Vectorization:  Binary vectorizer\n",
      "LinReg costfunction:  Regularized\n",
      "Precision:  0.86 \n",
      "Recall:  0.74 \n",
      "F1 score (micro): 0.80\n",
      "None\n",
      "Accuracy:  81.52%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          434   57\n",
      "1          121  351\n",
      "Data Clean:  Stemmed\n",
      "Vectorization:  Binary vectorizer\n",
      "LinReg costfunction:  Not regularized\n",
      "Precision:  0.86 \n",
      "Recall:  0.74 \n",
      "F1 score (micro): 0.80\n",
      "None\n",
      "Accuracy:  81.52%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          434   57\n",
      "1          121  351\n",
      "Data Clean:  Not Stemmed\n",
      "Vectorization:   TF-IDF vectorizer\n",
      "LinReg costfunction:  Regularized\n",
      "Precision:  0.82 \n",
      "Recall:  0.80 \n",
      "F1 score (micro): 0.81\n",
      "None\n",
      "Accuracy:  81.62%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          410   81\n",
      "1           96  376\n",
      "Data Clean:  Not Stemmed\n",
      "Vectorization:   TF-IDF vectorizer\n",
      "LinReg costfunction:  Not regularized\n",
      "Precision:  0.82 \n",
      "Recall:  0.80 \n",
      "F1 score (micro): 0.81\n",
      "None\n",
      "Accuracy:  81.62%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          410   81\n",
      "1           96  376\n",
      "Data Clean:  Not Stemmed\n",
      "Vectorization:   Count vectorizer\n",
      "LinReg costfunction:  Regularized\n",
      "Precision:  0.81 \n",
      "Recall:  0.75 \n",
      "F1 score (micro): 0.78\n",
      "None\n",
      "Accuracy:  79.13%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          410   81\n",
      "1          120  352\n",
      "Data Clean:  Not Stemmed\n",
      "Vectorization:   Count vectorizer\n",
      "LinReg costfunction:  Not regularized\n",
      "Precision:  0.81 \n",
      "Recall:  0.75 \n",
      "F1 score (micro): 0.78\n",
      "None\n",
      "Accuracy:  79.13%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          410   81\n",
      "1          120  352\n",
      "Data Clean:  Not Stemmed\n",
      "Vectorization:  Binary vectorizer\n",
      "LinReg costfunction:  Regularized\n",
      "Precision:  0.83 \n",
      "Recall:  0.73 \n",
      "F1 score (micro): 0.78\n",
      "None\n",
      "Accuracy:  79.34%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          421   70\n",
      "1          129  343\n",
      "Data Clean:  Not Stemmed\n",
      "Vectorization:  Binary vectorizer\n",
      "LinReg costfunction:  Not regularized\n",
      "Precision:  0.83 \n",
      "Recall:  0.73 \n",
      "F1 score (micro): 0.78\n",
      "None\n",
      "Accuracy:  79.34%\n",
      "Confusion Matrix:\n",
      " Predicted  0.0  1.0\n",
      "Actual             \n",
      "0          421   70\n",
      "1          129  343\n"
     ]
    }
   ],
   "source": [
    "def main(stemmed,vectorType,regularized):\n",
    "\n",
    "    #random.seed(123) #set seed\n",
    "  # loading the train set\n",
    "    pos_train_list = create_list(pos_train,\"train\", \"positive\")\n",
    "    neg_train_list = create_list(neg_train,\"train\", \"negative\")\n",
    "    X,y=createDataFrame(pos_train_list,neg_train_list)\n",
    "\n",
    "    #Checking Stemming/ Not Stemming data\n",
    "    if stemmed == 1:\n",
    "        for i in range(0, len(X)):\n",
    "            X[i] = cleaningStemmed(X[i])\n",
    "\n",
    "    else:\n",
    "        for i in range(0, len(X)):\n",
    "            X[i] = clean(X[i])\n",
    "\n",
    "    # Creating vocabulary list\n",
    "    vocab = X[0]\n",
    "    for i in range(1, len(X)):\n",
    "        vocab.extend(X[i])\n",
    "    vocab = sorted(set(vocab))\n",
    "\n",
    "    row = len(X)\n",
    "    col = len(vocab)\n",
    "\n",
    "    dict_vocab = {}\n",
    "    for i, j in enumerate(vocab):\n",
    "        dict_vocab[j] = i\n",
    "    trainVector = np.zeros((row, col), dtype=np.int64)\n",
    "\n",
    "    # Vectorizing the trainset\n",
    "    trainVector=vectorizer(X,trainVector,dict_vocab,vectorType,row,col)\n",
    "    m, n = trainVector.shape\n",
    "    trainVector = np.concatenate([np.ones((m, 1)), trainVector], axis=1)\n",
    "    #trainVector = preprocessing.scale(trainVector)\n",
    "\n",
    "    initial_theta = np.zeros(n + 1)\n",
    "    iterations = 1000\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # Logistic function\n",
    "    if regularized==1:\n",
    "        lmbda = 0.1\n",
    "        (cost_history, params_optimal) = gradient_descent_reg(trainVector, y, initial_theta, learning_rate, iterations,lmbda)\n",
    "    else :\n",
    "        (cost_history, params_optimal) = gradient_descent(trainVector, y, initial_theta, learning_rate, iterations)\n",
    "\n",
    " #Loading the test set\n",
    "    pos_test_list = create_list(pos_test, \"test\", \"positive\")\n",
    "    neg_test_list = create_list(neg_test, \"test\", \"negative\")\n",
    "    X_test, y_test = createDataFrame(pos_test_list, neg_test_list)\n",
    "\n",
    "    # Stemming data\n",
    "    if (stemmed == 1):\n",
    "        for i in range(0, len(X_test)):\n",
    "            X_test[i] = cleaningStemmed(X_test[i])\n",
    "\n",
    "    else:\n",
    "        for i in range(0, len(X_test)):\n",
    "            X_test[i] = clean(X_test[i])\n",
    "\n",
    "    row = len(X_test)\n",
    "    col = len(vocab)\n",
    "\n",
    "    testVector = np.zeros((row, col), dtype=np.int64)\n",
    "    # Vectorizing the test data\n",
    "    testVector = vectorizer(X_test,testVector,dict_vocab,vectorType,row,col)\n",
    "    m, n = testVector.shape\n",
    "\n",
    "    testVector=np.concatenate([np.ones((m, 1)), testVector], axis=1)\n",
    "    #testVector = preprocessing.scale(testVector)\n",
    "\n",
    "    # Final Prediction\n",
    "    preds = predict(testVector , params_optimal)\n",
    "\n",
    "    # Final values based on threshold value 0.5\n",
    "    for i in range(0, len(preds)):\n",
    "        if (preds[i] <= 0.5):\n",
    "            preds[i] = 0\n",
    "        else:\n",
    "            preds[i] = 1\n",
    "\n",
    "    if(stemmed==1):\n",
    "        dataClean=\"Stemmed\"\n",
    "    else:\n",
    "        dataClean = \"Not Stemmed\"\n",
    "\n",
    "    if (vectorType == 1):\n",
    "        type = \" TF-IDF vectorizer\"\n",
    "    elif (vectorType==2):\n",
    "        type = \" Count vectorizer\"\n",
    "    else:\n",
    "        type=\"Binary vectorizer\"\n",
    "\n",
    "    if (regularized==1):\n",
    "        reg=\"Regularized\"\n",
    "    else:\n",
    "        reg=\"Not regularized\"\n",
    "\n",
    "    # Output\n",
    "    print(\"Data Clean: \",dataClean)\n",
    "    print(\"Vectorization: \",type)\n",
    "    print(\"LinReg costfunction: \",reg)\n",
    "    #print(\"F1 SCore: \",f1_score(y_test,preds, average='macro'))\n",
    "    print(f1_score(confusion_matrix(y_test,preds)))\n",
    "    print(\"Accuracy: \",\"{:.2f}%\".format(accuracy_score(y_test,preds)))\n",
    "    print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,preds))\n",
    "if __name__ == \"__main__\" :\n",
    "   \n",
    "    for i in range(1, 3):\n",
    "        for j in range(1,4):\n",
    "            for k in range(1,3):\n",
    "                main(i,j,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9do37Us4tto6"
   },
   "source": [
    "### Bonus point: \n",
    "\n",
    "##### How would the results change if you used term frequency x inverse document frequency instead of binary representation for both logistic regression and Naïve Bayes (1 point)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gFR9w9yNr8i"
   },
   "source": [
    "For logistic regression, we found that using TF-IDF improved the  model accuracy compared to when just binary representation for both stemmed and not stemmed vocabulary of tweets. When TF-IDF was used for not stemmed, accuracy was 84.80% while with binary it was 82.50%. When TF-IDF was used for stemmed, accuracy was 86.40% while with Binary it was 83.90%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxPBck2KtzwW"
   },
   "source": [
    "##### How do your results change if you regularize your logistic regression (1 point)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xy6YPpfrNr8j"
   },
   "source": [
    "Adding regularization did not decrease accuracy with our models when implemented for binary representation, count vectorizer or Tf-IDF. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AIT726_Team_ShinojKumar_YitongLi_AnhNguyen_NinaNnamani_H1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
